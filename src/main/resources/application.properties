quarkus.langchain4j.ollama.llama3.timeout=180s
quarkus.langchain4j.ollama.llama3.chat-model.model-id=llama3.2
quarkus.langchain4j.ollama.llama3.chat-model.temperature=0.2
quarkus.langchain4j.ollama.llama3.chat-model.top-k=40
quarkus.langchain4j.ollama.llama3.chat-model.top-p=0.8
quarkus.langchain4j.log-requests=false
quarkus.langchain4j.log-responses=true

quarkus.http.test-timeout=120s